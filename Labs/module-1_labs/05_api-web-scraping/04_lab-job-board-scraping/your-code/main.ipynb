{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\"\"\"\n",
    "This function searches job posts from LinkedIn and converts the results into a dataframe.\n",
    "\"\"\"\n",
    "def scrape_linkedin_job_search(keywords):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Live Nation Entertainment</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>FRONT</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Front</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>Fremont, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Goldstone Partners, Inc.</td>\n",
       "      <td>Boulder, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Banyan Health Systems</td>\n",
       "      <td>Miami Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data Analyst, Business Analysis and Reporting ...</td>\n",
       "      <td>Publix Super Markets</td>\n",
       "      <td>Lakeland, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bradford &amp; Galt</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Fraud Data Analyst</td>\n",
       "      <td>Abercrombie &amp; Fitch Co.</td>\n",
       "      <td>Columbus, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Analytics Consultant</td>\n",
       "      <td>INNOVATO Solutions</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>MNG Health</td>\n",
       "      <td>Greater Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Google</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Analytics Analyst I</td>\n",
       "      <td>Modis</td>\n",
       "      <td>Wilmington, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Incept Data Solutions, Inc.</td>\n",
       "      <td>Sterling, VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Data Scientist, Workforce Analytics</td>\n",
       "      <td>Warner Bros. Entertainment</td>\n",
       "      <td>Burbank, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Engineering - a Hill International Company</td>\n",
       "      <td>Allendale, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Data &amp; Analytics Specialist</td>\n",
       "      <td>Veear Projects Inc.</td>\n",
       "      <td>South San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Capacity LLC</td>\n",
       "      <td>North Brunswick, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Twitch</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Business Intelligence Engineer - 2020 (United ...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Seattle, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>CNN</td>\n",
       "      <td>New York City Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Reliance One, Inc.</td>\n",
       "      <td>Georgetown, KY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cooper Tire &amp; Rubber Company</td>\n",
       "      <td>Findlay, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Woodland Foods</td>\n",
       "      <td>Waukegan, IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cortex Consultants</td>\n",
       "      <td>Summit, NJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                        Data Analyst   \n",
       "1                                        Data Analyst   \n",
       "2                                        Data Analyst   \n",
       "3                                        Data Analyst   \n",
       "4                                        Data Analyst   \n",
       "5                                        Data Analyst   \n",
       "6   Data Analyst, Business Analysis and Reporting ...   \n",
       "7                                        Data Analyst   \n",
       "8                                  Fraud Data Analyst   \n",
       "9                           Data Analytics Consultant   \n",
       "10                                       Data Analyst   \n",
       "11                                       Data Analyst   \n",
       "12                                Analytics Analyst I   \n",
       "13                                       Data Analyst   \n",
       "14                Data Scientist, Workforce Analytics   \n",
       "15                                Junior Data Analyst   \n",
       "16                       Data & Analytics Specialist    \n",
       "17                                       Data Analyst   \n",
       "18                                       Data Analyst   \n",
       "19  Business Intelligence Engineer - 2020 (United ...   \n",
       "20                                     Data Scientist   \n",
       "21                                       Data Analyst   \n",
       "22                                       Data Analyst   \n",
       "23                                       Data Analyst   \n",
       "24                                       Data Analyst   \n",
       "\n",
       "                                       Company  \\\n",
       "0                    Live Nation Entertainment   \n",
       "1                                        FRONT   \n",
       "2                                        Front   \n",
       "3                                        Tesla   \n",
       "4                     Goldstone Partners, Inc.   \n",
       "5                        Banyan Health Systems   \n",
       "6                         Publix Super Markets   \n",
       "7                              Bradford & Galt   \n",
       "8                      Abercrombie & Fitch Co.   \n",
       "9                           INNOVATO Solutions   \n",
       "10                                  MNG Health   \n",
       "11                                      Google   \n",
       "12                                       Modis   \n",
       "13                 Incept Data Solutions, Inc.   \n",
       "14                  Warner Bros. Entertainment   \n",
       "15  Engineering - a Hill International Company   \n",
       "16                         Veear Projects Inc.   \n",
       "17                                Capacity LLC   \n",
       "18                                      Twitch   \n",
       "19                                      Amazon   \n",
       "20                                         CNN   \n",
       "21                          Reliance One, Inc.   \n",
       "22                Cooper Tire & Rubber Company   \n",
       "23                              Woodland Foods   \n",
       "24                          Cortex Consultants   \n",
       "\n",
       "                           Location  \n",
       "0                   Los Angeles, CA  \n",
       "1                 San Francisco, CA  \n",
       "2                 San Francisco, CA  \n",
       "3                       Fremont, CA  \n",
       "4                       Boulder, CO  \n",
       "5           Miami Metropolitan Area  \n",
       "6                      Lakeland, FL  \n",
       "7                      St Louis, MO  \n",
       "8                      Columbus, OH  \n",
       "9                 San Francisco, CA  \n",
       "10             Greater Philadelphia  \n",
       "11                Mountain View, CA  \n",
       "12                   Wilmington, DE  \n",
       "13                     Sterling, VA  \n",
       "14                      Burbank, CA  \n",
       "15                    Allendale, NJ  \n",
       "16          South San Francisco, CA  \n",
       "17              North Brunswick, NJ  \n",
       "18                San Francisco, CA  \n",
       "19                      Seattle, WA  \n",
       "20  New York City Metropolitan Area  \n",
       "21                   Georgetown, KY  \n",
       "22                      Findlay, OH  \n",
       "23                     Waukegan, IL  \n",
       "24                       Summit, NJ  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to call the function\n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?keywords=data%20analysis&start=50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Live Nation Entertainment</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>FRONT</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Front</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>Fremont, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Goldstone Partners, Inc.</td>\n",
       "      <td>Boulder, CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Banyan Health Systems</td>\n",
       "      <td>Miami Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data Analyst, Business Analysis and Reporting ...</td>\n",
       "      <td>Publix Super Markets</td>\n",
       "      <td>Lakeland, FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bradford &amp; Galt</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Fraud Data Analyst</td>\n",
       "      <td>Abercrombie &amp; Fitch Co.</td>\n",
       "      <td>Columbus, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Analytics Consultant</td>\n",
       "      <td>INNOVATO Solutions</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>MNG Health</td>\n",
       "      <td>Greater Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Google</td>\n",
       "      <td>Mountain View, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Analytics Analyst I</td>\n",
       "      <td>Modis</td>\n",
       "      <td>Wilmington, DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Data Scientist, Workforce Analytics</td>\n",
       "      <td>Warner Bros. Entertainment</td>\n",
       "      <td>Burbank, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Incept Data Solutions, Inc.</td>\n",
       "      <td>Sterling, VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Engineering - a Hill International Company</td>\n",
       "      <td>Allendale, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Data &amp; Analytics Specialist</td>\n",
       "      <td>Veear Projects Inc.</td>\n",
       "      <td>South San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>AccumTech</td>\n",
       "      <td>Livonia, MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>BECAUSE LLC</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Capacity LLC</td>\n",
       "      <td>North Brunswick, NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Twitch</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Business Intelligence Engineer - 2020 (United ...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Seattle, WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>CNN</td>\n",
       "      <td>New York City Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Reliance One, Inc.</td>\n",
       "      <td>Georgetown, KY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Cooper Tire &amp; Rubber Company</td>\n",
       "      <td>Findlay, OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                        Data Analyst   \n",
       "1                                        Data Analyst   \n",
       "2                                        Data Analyst   \n",
       "3                                        Data Analyst   \n",
       "4                                        Data Analyst   \n",
       "5                                        Data Analyst   \n",
       "6   Data Analyst, Business Analysis and Reporting ...   \n",
       "7                                        Data Analyst   \n",
       "8                                  Fraud Data Analyst   \n",
       "9                           Data Analytics Consultant   \n",
       "10                                       Data Analyst   \n",
       "11                                       Data Analyst   \n",
       "12                                Analytics Analyst I   \n",
       "13                Data Scientist, Workforce Analytics   \n",
       "14                                       Data Analyst   \n",
       "15                                Junior Data Analyst   \n",
       "16                       Data & Analytics Specialist    \n",
       "17                                       Data Analyst   \n",
       "18                                       Data Analyst   \n",
       "19                                       Data Analyst   \n",
       "20                                       Data Analyst   \n",
       "21  Business Intelligence Engineer - 2020 (United ...   \n",
       "22                                     Data Scientist   \n",
       "23                                       Data Analyst   \n",
       "24                                       Data Analyst   \n",
       "\n",
       "                                       Company  \\\n",
       "0                    Live Nation Entertainment   \n",
       "1                                        FRONT   \n",
       "2                                        Front   \n",
       "3                                        Tesla   \n",
       "4                     Goldstone Partners, Inc.   \n",
       "5                        Banyan Health Systems   \n",
       "6                         Publix Super Markets   \n",
       "7                              Bradford & Galt   \n",
       "8                      Abercrombie & Fitch Co.   \n",
       "9                           INNOVATO Solutions   \n",
       "10                                  MNG Health   \n",
       "11                                      Google   \n",
       "12                                       Modis   \n",
       "13                  Warner Bros. Entertainment   \n",
       "14                 Incept Data Solutions, Inc.   \n",
       "15  Engineering - a Hill International Company   \n",
       "16                         Veear Projects Inc.   \n",
       "17                                   AccumTech   \n",
       "18                                 BECAUSE LLC   \n",
       "19                                Capacity LLC   \n",
       "20                                      Twitch   \n",
       "21                                      Amazon   \n",
       "22                                         CNN   \n",
       "23                          Reliance One, Inc.   \n",
       "24                Cooper Tire & Rubber Company   \n",
       "\n",
       "                           Location  \n",
       "0                   Los Angeles, CA  \n",
       "1                 San Francisco, CA  \n",
       "2                 San Francisco, CA  \n",
       "3                       Fremont, CA  \n",
       "4                       Boulder, CO  \n",
       "5           Miami Metropolitan Area  \n",
       "6                      Lakeland, FL  \n",
       "7                      St Louis, MO  \n",
       "8                      Columbus, OH  \n",
       "9                 San Francisco, CA  \n",
       "10             Greater Philadelphia  \n",
       "11                Mountain View, CA  \n",
       "12                   Wilmington, DE  \n",
       "13                      Burbank, CA  \n",
       "14                     Sterling, VA  \n",
       "15                    Allendale, NJ  \n",
       "16          South San Francisco, CA  \n",
       "17                      Livonia, MI  \n",
       "18                 Redwood City, CA  \n",
       "19              North Brunswick, NJ  \n",
       "20                San Francisco, CA  \n",
       "21                      Seattle, WA  \n",
       "22  New York City Metropolitan Area  \n",
       "23                   Georgetown, KY  \n",
       "24                      Findlay, OH  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_linkedin_job_search(keywords, num_pages):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    scrape_urls = []\n",
    "    # Assemble the full url with parameters\n",
    "    for x in range(num_pages):\n",
    "        if x == 0:\n",
    "            scrape_urls.append(''.join([BASE_URL, 'keywords=', keywords]))\n",
    "        else:\n",
    "            scrape_urls.append(''.join([BASE_URL, 'keywords=', keywords, '&start=', str(num_pages * 25)]))\n",
    "            \n",
    "    for scrape_url in scrape_urls:\n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "        page = requests.get(scrape_url)\n",
    "        if page.status_code == requests.codes.ok:\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    print(scrape_url)\n",
    "    # Return dataframe\n",
    "    return data\n",
    "    \n",
    "\n",
    "results = scrape_linkedin_job_search('data%20analysis', 2)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_job_search(keywords, country):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&location=', country])\n",
    "    \n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    print(scrape_url)\n",
    "    # Return dataframe\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?keywords=data%20analysis&location=München\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(Junior) Data Scientist / Data Analyst (w/m/d)</td>\n",
       "      <td>KPMG Deutschland</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data Analyst / Data Engineer (m/w/d) - uptodate</td>\n",
       "      <td>Bridgemaker GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Data Analyst (m/f/x)</td>\n",
       "      <td>AutoScout24</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Intern Market Research &amp; Data Analysis (m/f/x)</td>\n",
       "      <td>Swiss Re</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data Analyst (m/w/d)</td>\n",
       "      <td>IGEL Projekt Service GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Analyst (m/f/d)</td>\n",
       "      <td>Allianz Global Corporate &amp; Specialty (AGCS)</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>FlixBus</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Allianz SE (global headquarters)</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Applied Data Scientist (m/f/d)</td>\n",
       "      <td>Allianz Technology</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Investment Data Analyst (m/w/d)</td>\n",
       "      <td>Allianz</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Data Analyst / Data Engineer (m/f/d)</td>\n",
       "      <td>uptodate Ventures GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Analytics Consultant : Munich</td>\n",
       "      <td>InterWorks</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Data Scientist (m/f/x)</td>\n",
       "      <td>Wirecard</td>\n",
       "      <td>Aschheim, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Property Analyst Data &amp; Process Management (m/w/d</td>\n",
       "      <td>Deutsche Pfandbriefbank AG</td>\n",
       "      <td>Garching, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>GO! Graduate IT &amp; Data Analytics (f/m/d) - Ger...</td>\n",
       "      <td>Essity</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Data analyst/ Data manager (f/m/d)</td>\n",
       "      <td>German Entrepreneurship GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Data Analyst / Engineer (m/w/d)</td>\n",
       "      <td>puren</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Data Analyst (m/w/d)</td>\n",
       "      <td>Abalon AB</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Predictive Insights Engine (m/w/d)</td>\n",
       "      <td>Global Cross Sourcing by Synergie</td>\n",
       "      <td>Haar, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Junior Project Manager (d/f/m)</td>\n",
       "      <td>Amway Europe</td>\n",
       "      <td>Greater Munich Metropolitan Area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Data Analyst (m/w/d)</td>\n",
       "      <td>BayWa AG</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Data Scientist (m/w/div.)</td>\n",
       "      <td>DMG MORI</td>\n",
       "      <td>Geretsried, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Junior Business Analyst/ Data Analyst</td>\n",
       "      <td>eSAR GmbH</td>\n",
       "      <td>Munich, Bavaria, Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  \\\n",
       "0                                        Data Analyst   \n",
       "1      (Junior) Data Scientist / Data Analyst (w/m/d)   \n",
       "2     Data Analyst / Data Engineer (m/w/d) - uptodate   \n",
       "3                                Data Analyst (m/f/x)   \n",
       "4                              Data Scientist (m/f/d)   \n",
       "5      Intern Market Research & Data Analysis (m/f/x)   \n",
       "6                                Data Analyst (m/w/d)   \n",
       "7                                Data Analyst (m/f/d)   \n",
       "8                              Data Scientist (m/f/d)   \n",
       "9                                        Data Analyst   \n",
       "10                     Applied Data Scientist (m/f/d)   \n",
       "11                    Investment Data Analyst (m/w/d)   \n",
       "12               Data Analyst / Data Engineer (m/f/d)   \n",
       "13                      Analytics Consultant : Munich   \n",
       "14                             Data Scientist (m/f/x)   \n",
       "15  Property Analyst Data & Process Management (m/w/d   \n",
       "16  GO! Graduate IT & Data Analytics (f/m/d) - Ger...   \n",
       "17                 Data analyst/ Data manager (f/m/d)   \n",
       "18                    Data Analyst / Engineer (m/w/d)   \n",
       "19                               Data Analyst (m/w/d)   \n",
       "20                 Predictive Insights Engine (m/w/d)   \n",
       "21                     Junior Project Manager (d/f/m)   \n",
       "22                               Data Analyst (m/w/d)   \n",
       "23                          Data Scientist (m/w/div.)   \n",
       "24              Junior Business Analyst/ Data Analyst   \n",
       "\n",
       "                                        Company  \\\n",
       "0                                        Amazon   \n",
       "1                              KPMG Deutschland   \n",
       "2                              Bridgemaker GmbH   \n",
       "3                                   AutoScout24   \n",
       "4                                        Huawei   \n",
       "5                                      Swiss Re   \n",
       "6                     IGEL Projekt Service GmbH   \n",
       "7   Allianz Global Corporate & Specialty (AGCS)   \n",
       "8                                       FlixBus   \n",
       "9              Allianz SE (global headquarters)   \n",
       "10                           Allianz Technology   \n",
       "11                                      Allianz   \n",
       "12                       uptodate Ventures GmbH   \n",
       "13                                   InterWorks   \n",
       "14                                     Wirecard   \n",
       "15                   Deutsche Pfandbriefbank AG   \n",
       "16                                       Essity   \n",
       "17                 German Entrepreneurship GmbH   \n",
       "18                                        puren   \n",
       "19                                    Abalon AB   \n",
       "20            Global Cross Sourcing by Synergie   \n",
       "21                                 Amway Europe   \n",
       "22                                     BayWa AG   \n",
       "23                                     DMG MORI   \n",
       "24                                    eSAR GmbH   \n",
       "\n",
       "                            Location  \n",
       "0           Munich, Bavaria, Germany  \n",
       "1           Munich, Bavaria, Germany  \n",
       "2           Munich, Bavaria, Germany  \n",
       "3           Munich, Bavaria, Germany  \n",
       "4           Munich, Bavaria, Germany  \n",
       "5           Munich, Bavaria, Germany  \n",
       "6           Munich, Bavaria, Germany  \n",
       "7           Munich, Bavaria, Germany  \n",
       "8           Munich, Bavaria, Germany  \n",
       "9           Munich, Bavaria, Germany  \n",
       "10          Munich, Bavaria, Germany  \n",
       "11          Munich, Bavaria, Germany  \n",
       "12          Munich, Bavaria, Germany  \n",
       "13          Munich, Bavaria, Germany  \n",
       "14        Aschheim, Bavaria, Germany  \n",
       "15        Garching, Bavaria, Germany  \n",
       "16          Munich, Bavaria, Germany  \n",
       "17          Munich, Bavaria, Germany  \n",
       "18          Munich, Bavaria, Germany  \n",
       "19          Munich, Bavaria, Germany  \n",
       "20            Haar, Bavaria, Germany  \n",
       "21  Greater Munich Metropolitan Area  \n",
       "22          Munich, Bavaria, Germany  \n",
       "23      Geretsried, Bavaria, Germany  \n",
       "24          Munich, Bavaria, Germany  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search('data%20analysis', 'München')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def scrape_linkedin_job_search(keywords, country, num_days):\n",
    "    \n",
    "    # Define the base url to be scraped.\n",
    "    # All uppercase variable name signifies this is a constant and its value should never unchange\n",
    "    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n",
    "    \n",
    "    seconds = num_days * 86400 \n",
    "    \n",
    "    # Assemble the full url with parameters\n",
    "    scrape_url = ''.join([BASE_URL, 'keywords=', keywords, '&location=', country, '&f_TPR=r', str(seconds)])\n",
    "    \n",
    "\n",
    "    # Create a request to get the data from the server \n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Create an empty dataframe with the columns consisting of the information you want to capture\n",
    "    columns = ['Title', 'Company', 'Location']\n",
    "    data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n",
    "    # Then in each job card, extract the job title, company, and location data.\n",
    "    titles = []\n",
    "    companies = []\n",
    "    locations = []\n",
    "    for card in soup.select(\"div.result-card__contents\"):\n",
    "        title = card.findChild(\"h3\", recursive=False)\n",
    "        company = card.findChild(\"h4\", recursive=False)\n",
    "        location = card.findChild(\"span\", attrs={\"class\": \"job-result-card__location\"}, recursive=True)\n",
    "        titles.append(title.string)\n",
    "        companies.append(company.string)\n",
    "        locations.append(location.string)\n",
    "    \n",
    "    # Inject job titles, companies, and locations into the empty dataframe\n",
    "    zipped = zip(titles, companies, locations)\n",
    "    for z in list(zipped):\n",
    "        data=data.append({'Title' : z[0] , 'Company' : z[1], 'Location': z[2]} , ignore_index=True)\n",
    "    \n",
    "    print(scrape_url)\n",
    "    \n",
    "    # Return dataframe\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?keywords=data%20analysis&location=Brazil&f_TPR=r259200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Analista Data Analytics</td>\n",
       "      <td>PwC Brasil</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>AMARO</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Consultant, Data Analytics</td>\n",
       "      <td>Control Risks</td>\n",
       "      <td>Subprefecture of Sé, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Engenheiro de Dados</td>\n",
       "      <td>Conquest One</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Zé Delivery</td>\n",
       "      <td>Greater Sao Paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Analista de BI</td>\n",
       "      <td>Flag Comunicação</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Especialista em Planejamento e Análise de Dados</td>\n",
       "      <td>VR Beneficios</td>\n",
       "      <td>São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Data Analyst - People Analytics</td>\n",
       "      <td>iFood</td>\n",
       "      <td>Campinas, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Cientista de dados</td>\n",
       "      <td>Hypera</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Zoox Smart Data</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Garena</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Statistical Modeler (Insurance)</td>\n",
       "      <td>LexisNexis Risk Solutions</td>\n",
       "      <td>São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Cientista de dados</td>\n",
       "      <td>Pottencial Seguradora S.A</td>\n",
       "      <td>Belo Horizonte, Minas Gerais, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Engenheiro de dados</td>\n",
       "      <td>Ewave do Brasil</td>\n",
       "      <td>Curitiba, Paraná, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>99</td>\n",
       "      <td>Greater Sao Paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>IFESP</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>inGaia</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Especialista de Analytics</td>\n",
       "      <td>Casa &amp; Video</td>\n",
       "      <td>Greater Rio de Janeiro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Howmet</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Digital Analytics</td>\n",
       "      <td>Certisign</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Business Analytics Analyst</td>\n",
       "      <td>Loft</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Azion</td>\n",
       "      <td>Porto Alegre, Rio Grande do Sul, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Liv Up</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Business Intelligence Intern - Brazil Based</td>\n",
       "      <td>Khan Academy</td>\n",
       "      <td>Fortaleza, Ceará, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Analista de BI</td>\n",
       "      <td>Docket Brasil</td>\n",
       "      <td>São Paulo, São Paulo, Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "0                           Analista Data Analytics   \n",
       "1                                      Data Analyst   \n",
       "2                        Consultant, Data Analytics   \n",
       "3                               Engenheiro de Dados   \n",
       "4                                      Data Analyst   \n",
       "5                                    Analista de BI   \n",
       "6   Especialista em Planejamento e Análise de Dados   \n",
       "7                   Data Analyst - People Analytics   \n",
       "8                                Cientista de dados   \n",
       "9                                      Data Analyst   \n",
       "10                                   Data Scientist   \n",
       "11                  Statistical Modeler (Insurance)   \n",
       "12                               Cientista de dados   \n",
       "13                              Engenheiro de dados   \n",
       "14                                     Data Analyst   \n",
       "15                                     Data Analyst   \n",
       "16                                     Data Analyst   \n",
       "17                        Especialista de Analytics   \n",
       "18                                     Data Analyst   \n",
       "19                                Digital Analytics   \n",
       "20                       Business Analytics Analyst   \n",
       "21                                     Data Analyst   \n",
       "22                                     Data Analyst   \n",
       "23      Business Intelligence Intern - Brazil Based   \n",
       "24                                   Analista de BI   \n",
       "\n",
       "                      Company                                 Location  \n",
       "0                  PwC Brasil             São Paulo, São Paulo, Brazil  \n",
       "1                       AMARO             São Paulo, São Paulo, Brazil  \n",
       "2               Control Risks   Subprefecture of Sé, São Paulo, Brazil  \n",
       "3                Conquest One             São Paulo, São Paulo, Brazil  \n",
       "4                 Zé Delivery                        Greater Sao Paulo  \n",
       "5            Flag Comunicação             São Paulo, São Paulo, Brazil  \n",
       "6               VR Beneficios                        São Paulo, Brazil  \n",
       "7                       iFood              Campinas, São Paulo, Brazil  \n",
       "8                      Hypera             São Paulo, São Paulo, Brazil  \n",
       "9             Zoox Smart Data             São Paulo, São Paulo, Brazil  \n",
       "10                     Garena             São Paulo, São Paulo, Brazil  \n",
       "11  LexisNexis Risk Solutions                        São Paulo, Brazil  \n",
       "12  Pottencial Seguradora S.A     Belo Horizonte, Minas Gerais, Brazil  \n",
       "13            Ewave do Brasil                 Curitiba, Paraná, Brazil  \n",
       "14                         99                        Greater Sao Paulo  \n",
       "15                      IFESP             São Paulo, São Paulo, Brazil  \n",
       "16                     inGaia             São Paulo, São Paulo, Brazil  \n",
       "17               Casa & Video                   Greater Rio de Janeiro  \n",
       "18                     Howmet             São Paulo, São Paulo, Brazil  \n",
       "19                  Certisign             São Paulo, São Paulo, Brazil  \n",
       "20                       Loft             São Paulo, São Paulo, Brazil  \n",
       "21                      Azion  Porto Alegre, Rio Grande do Sul, Brazil  \n",
       "22                     Liv Up             São Paulo, São Paulo, Brazil  \n",
       "23               Khan Academy                 Fortaleza, Ceará, Brazil  \n",
       "24              Docket Brasil             São Paulo, São Paulo, Brazil  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = scrape_linkedin_job_search('data%20analysis', 'Brazil', 3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
